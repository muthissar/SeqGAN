{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/30/0/70339/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/zhome/30/0/70339/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/zhome/30/0/70339/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/zhome/30/0/70339/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/zhome/30/0/70339/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/zhome/30/0/70339/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabular_simple import TabularSimple\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from rollout_max_ent import ROLLOUT\n",
    "import tensorflow as tf\n",
    "from gan_trainer import GanTrainer\n",
    "from dataloader import Gen_Data_loader, Dis_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modes = 2\n",
    "n_vocabulary = 4\n",
    "vocab = range(n_vocabulary)\n",
    "tabular_model = TabularSimple(4,n_vocabulary,n_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'10'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0c0cc1937f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabular_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"10\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '10'"
     ]
    }
   ],
   "source": [
    "dist = tabular_model.table[\"10\"]\n",
    "plt.bar(vocab, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "samples = tabular_model.sample(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('save/real_data_tab.txt',samples,fmt='%d',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(tabular_model.table.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model.ll(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TabularSimple(4,n_vocabulary,n_modes).ll(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Generator  Hyper-parameters\n",
    "######################################################################################\n",
    "EMB_DIM = 4 # embedding dimension\n",
    "HIDDEN_DIM = 4 # hidden state dimension of lstm cell\n",
    "SEQ_LENGTH = 4 # sequence length\n",
    "START_TOKEN = 0\n",
    "PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n",
    "SEED = 88\n",
    "BATCH_SIZE = 128\n",
    "vocab_size = 4\n",
    "\n",
    "#########################################################################################\n",
    "#  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 4\n",
    "dis_filter_sizes = [1, 2, 3, 4]\n",
    "#dis_num_filters = [200, 200, 200, 200]\n",
    "dis_num_filters = [10, 10, 10, 10]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 128\n",
    "\n",
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "#########################################################################################\n",
    "TOTAL_BATCH = 200\n",
    "positive_file = 'save/real_data_tab.txt'\n",
    "negative_file = 'save/generator_sample_tab.txt'\n",
    "negative_file_ent = 'save/generator_sample_tab_ent.txt'\n",
    "#eval_file = 'save/eval_file_tab.txt'\n",
    "generated_num = 10000\n",
    "sequence_length = 4\n",
    "g_lr = 0.01\n",
    "generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN,learning_rate=g_lr)\n",
    "generator_ent = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN,learning_rate=g_lr)\n",
    "\n",
    "discriminator = Discriminator(sequence_length=sequence_length, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                            filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n",
    "rollout = ROLLOUT(generator, 0.8)\n",
    "rollout_ent = ROLLOUT(generator_ent, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_loader = Gen_Data_loader(BATCH_SIZE,SEQ_LENGTH)\n",
    "gen_data_loader_ent = Gen_Data_loader(BATCH_SIZE,SEQ_LENGTH)\n",
    "dis_data_loader = Dis_dataloader(BATCH_SIZE,SEQ_LENGTH)\n",
    "gan_trainer = GanTrainer(generator,discriminator,rollout,gen_data_loader,dis_data_loader,\n",
    "           tabular_model,'pretrain_notebook','advtrain_notebook',positive_file,negative_file,BATCH_SIZE)\n",
    "gan_trainer_ent = GanTrainer(generator_ent,discriminator,rollout_ent,gen_data_loader_ent,dis_data_loader,\n",
    "           tabular_model,'pretrain_notebook','advtrain_notebook',positive_file,negative_file_ent,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run from saved checkpoint \n",
    "#saver = tf.train.Saver()\n",
    "#tf.reset_default_graph()\n",
    "#saver.restore(sess, 'model/pretrain_max_ent_tab.ckpt')\n",
    "#saver.restore(sess, 'model/advtrain.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_trainer.pretrain(sess, 20, 20,3,\n",
    "    saver,dis_dropout_keep_prob,generated_num)\n",
    "gan_trainer_ent.pretrain(sess, 20, 20,3,\n",
    "    saver,dis_dropout_keep_prob,generated_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVEN WITH A VERY HIGH ENTROPY CONSTANT WE DON't see any major difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(10000):\n",
    "    for temp, gen, gan in zip([9999,.25],[generator,generator_ent],[gan_trainer, gan_trainer_ent]):\n",
    "        test_loss, g_loss = gan.advtrain_gen(sess,1,64,temp)\n",
    "        policy_ent = sess.run(gen.pretrain_loss,\n",
    "                {gen.x: gen.generate(sess)})\n",
    "        class_ = 1\n",
    "        predictions = np.array([])\n",
    "        for i in range(10):\n",
    "            predictions = np.concatenate((predictions,sess.run(discriminator.ypred_for_auc, {discriminator.input_x: gen.generate(sess), discriminator.dropout_keep_prob: dis_dropout_keep_prob})[:,class_]))\n",
    "        #self.writer.add_scalar('Loss/discrim_loss', disc_loss, total_batch)\n",
    "        #print(\"discrim  --  min: {}, max: {}, ll: {}, loss: {}\".format(min(predictions),max(predictions),np.mean(np.log(predictions)),disc_loss))\n",
    "        if it % 10 == 0:\n",
    "            print(\"GenT: {:.4f} -  test_loss: {:.4f}, g_loss: {:.4f}, pol_ent: {:.4f}, ll_disc: {:.4f}, maxp_disc: {:.4f}, minp_disc: {:.4f}\"\n",
    "                .format(temp, test_loss, g_loss,policy_ent,np.mean(np.log(predictions)),max(predictions),min(predictions)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ll is not affected too much by training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = tf.nn.embedding_lookup(generator.g_embeddings, [10]*128)\n",
    "h_tm1 = generator.h0\n",
    "h_t = generator.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
    "o_t = generator.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
    "#log_prob = tf.log(tf.nn.softmax(o_t))\n",
    "dist0 = sess.run(tf.nn.softmax(o_t))[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(vocab, dist0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = tf.nn.embedding_lookup(generator_ent.g_embeddings, [10]*128)\n",
    "h_tm1 = generator_ent.h0\n",
    "h_t = generator_ent.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
    "o_t = generator_ent.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
    "#log_prob = tf.log(tf.nn.softmax(o_t))\n",
    "dist0_ent = sess.run(tf.nn.softmax(o_t))[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(vocab, dist0_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(vocab,tabular_model.table[\"10\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
