#GPU : '0' # if you have 2 GPU, use '0' or '1'

#########################################################################################
#  Generator  Hyper-parameters
######################################################################################
EMB_DIM : 32 # embedding dimension
HIDDEN_DIM : 32 # hidden state dimension of lstm cell
SEQ_LENGTH : 20 # sequence length
START_TOKEN : 0

PRE_GEN_EPOCH : 120 # supervise (maximum likelihood estimation) epochs for generator (default: 120)
PRE_DIS_EPOCH : 50 # supervise (maximum likelihood estimation) epochs for discriminator (default: 50)
SEED : 88
BATCH_SIZE : 64
generator_lr : 0.01
ROLLOUT_UPDATE_RATE: 0.8
reward_gamma: 0.99
# use x10 learning rate for adversarial training: mainly for slow & accurate pretraining & more weighted adv training
x10adv_g: True
#########################################################################################
#  Discriminator  Hyper-parameters
#########################################################################################
dis_embedding_dim : 64
dis_filter_sizes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]
dis_num_filters: [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]
dis_dropout_keep_prob: 0.75
dis_l2_reg_lambda: 0.2
dis_batch_size: 64
rollout_num : 32
discriminator_lr : 0.0001
#########################################################################################
#  Basic Training Parameters
#########################################################################################
TOTAL_BATCH : 200
# vocab size for our custom data
vocab_size : 5000
# positive data, containing real music sequences
positive_file : 'save/real_data.txt'
# negative data from the generator, containing fake sequences
# specify different name if experimenting with multiple instances: causes EOF error & writing to same file from different instances
negative_file : 'save/generator_sample.txt'
valid_file : 'save/eval_file.txt'
# # of real data tokens is 140000
# specify so that generated_num * seq_length = 140000 to match balanced real & fake data
generated_num : 10000

epochs_generator : 1
epochs_discriminator : 5
epochs_discriminator_multiplier : 3

pretrain : True
# RL is stochastic: scrap and restart from pretrained checkpoint if things start failing
infinite_loop: True
# our dataset achives around 0.53 from pretraining
loop_threshold: 0.5
runmode_pretrain: 'fresh'
pretrain_file: 'model/pretrain_max_ent_lstm.ckpt'
runmode_advtrain: 'fresh'
advtrain_file: 'model/advtrain_max_ent_lstm.ckpt'
#Entropy temperature (controls how much entropy affects the reward)
#1 is normal 2 is halfed 
ent_temp: 1

is_music_data: False

target_class: TARGET_LSTM
target_args: 'save/target_params_py3.pkl'

number_model_save: 0
rewards_normalize: True